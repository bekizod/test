{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a791cc-68ff-4272-a270-dc58c2dee807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b217972f-4c38-4251-9709-919d167e5cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and preprocessor loaded successfully!\n",
      "Best Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best Accuracy: 0.9472131147540983\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.96      1.00      0.98        26\n",
      "           0       1.00      0.79      0.88        14\n",
      "           1       0.95      1.00      0.97        36\n",
      "\n",
      "    accuracy                           0.96        76\n",
      "   macro avg       0.97      0.93      0.94        76\n",
      "weighted avg       0.96      0.96      0.96        76\n",
      "\n",
      "Confusion Matrix:\n",
      " [[26  0  0]\n",
      " [ 1 11  2]\n",
      " [ 0  0 36]]\n",
      "Model Accuracy: 0.9605263157894737\n",
      "Model saved to model.pkl\n",
      "Preprocessor saved to preprocessor.pkl\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "window.open(\"http://127.0.0.1:8000\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Will watch for changes in these directories: ['C:\\\\Users\\\\Mnabah Tafetaleh\\\\nl']\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "INFO:     Started reloader process [7024] using StatReload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import uvicorn\n",
    "import pickle\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import os\n",
    "\n",
    "# Global preprocessor and model definition\n",
    "preprocessor = None\n",
    "model = None\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "def preprocess_data(df):\n",
    "    global preprocessor\n",
    "    # Drop unnecessary columns\n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(columns=[col for col in [\n",
    "        'timestamp', 'date_GMT', 'home_team_shots', 'over_35_percentage_pre_match', \n",
    "        'away_ppg', 'home_team_goal_count_half_time', 'over_25_percentage_pre_match', \n",
    "        'status', 'over_05_HT_FHG_percentage_pre_match', 'odds_ft_home_team_win', \n",
    "        'home_team_corner_count', 'odds_ft_over25', 'Away Team Pre-Match xG', \n",
    "        'Pre-Match PPG (Away)', 'away_team_first_half_cards', 'attendance.1', \n",
    "        'home_team_goal_timings', 'over_15_2HG_percentage_pre_match', \n",
    "        'odds_ft_away_team_win', 'home_ppg', 'Game Week', 'away_team_goal_timings', \n",
    "        'away_team_second_half_cards', 'Pre-Match PPG (Home)', 'attendance', \n",
    "        'odds_ft_draw', 'away_team_goal_count_half_time', 'btts_percentage_pre_match', \n",
    "        'odds_btts_yes', 'stadium_name', 'team_a_xg', 'home_team_first_half_cards', \n",
    "        'home_team_shots_off_target', 'Home Team Pre-Match xG', 'odds_ft_over35', \n",
    "        'odds_ft_over45', 'referee', 'home_team_second_half_cards', \n",
    "        'over_45_percentage_pre_match', 'away_team_name', 'away_team_corner_count', \n",
    "        'away_team_shots', 'over_05_2HG_percentage_pre_match', \n",
    "        'over_15_percentage_pre_match', 'average_cards_per_match_pre_match', \n",
    "        'average_corners_per_match_pre_match', 'odds_ft_over15', \n",
    "        'average_goals_per_match_pre_match', 'away_team_shots_off_target', \n",
    "        'total_goals_at_half_time', 'odds_btts_no', 'total_goal_count', 'team_b_xg', \n",
    "        'over_15_HT_FHG_percentage_pre_match'\n",
    "    ] if col in df.columns])\n",
    "\n",
    "    # Creating new features\n",
    "    df['possession_difference'] = df['home_team_possession'] - df['away_team_possession']\n",
    "    df['total_shots_on_target'] = df['home_team_shots_on_target'] + df['away_team_shots_on_target']\n",
    "\n",
    "    # Handle missing values in numeric columns\n",
    "    numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "    df[numeric_columns] = df[numeric_columns].fillna(0)\n",
    "\n",
    "    # Convert all numeric columns to float (to handle mixed types)\n",
    "    df[numeric_columns] = df[numeric_columns].astype(float)\n",
    "\n",
    "    # Encoding match results as labels\n",
    "    df['match_result'] = df.apply(lambda row: 1 if row['home_team_goal_count'] > row['away_team_goal_count']\n",
    "                                  else (-1 if row['home_team_goal_count'] < row['away_team_goal_count'] else 0), axis=1)\n",
    "\n",
    "    # Select numerical and categorical features\n",
    "    numerical_features = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    numerical_features.remove('match_result')  # Exclude target variable\n",
    "\n",
    "    categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Initialize the preprocessor here\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "        ])\n",
    "    \n",
    "    return df, numerical_features, categorical_features\n",
    "\n",
    "\n",
    "# Step 2: Model Training\n",
    "def train_model(X_train, y_train):\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "    return best_model\n",
    "\n",
    "# Step 3: Model Evaluation\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Step 4: Save Model and Preprocessor\n",
    "def save_model_and_preprocessor(model, preprocessor, model_filename=\"model.pkl\", preprocessor_filename=\"preprocessor.pkl\"):\n",
    "    with open(model_filename, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    with open(preprocessor_filename, \"wb\") as f:\n",
    "        pickle.dump(preprocessor, f)\n",
    "    print(f\"Model saved to {model_filename}\")\n",
    "    print(f\"Preprocessor saved to {preprocessor_filename}\")\n",
    "\n",
    "# Step 5: FastAPI App\n",
    "app = FastAPI()\n",
    "\n",
    "# Load trained model and preprocessor\n",
    "if os.path.exists(\"model.pkl\") and os.path.exists(\"preprocessor.pkl\"):\n",
    "    with open(\"model.pkl\", \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    with open(\"preprocessor.pkl\", \"rb\") as f:\n",
    "        preprocessor = pickle.load(f)\n",
    "    print(\"Model and preprocessor loaded successfully!\")\n",
    "else:\n",
    "    print(\"Model or preprocessor not found! Please train the model first.\")\n",
    "    model = None\n",
    "    preprocessor = None\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "async def predict(data: dict):\n",
    "    if model is None:\n",
    "        raise HTTPException(status_code=500, detail=\"Model not loaded. Train the model first.\")\n",
    "    if preprocessor is None:\n",
    "        raise HTTPException(status_code=500, detail=\"Preprocessor not loaded. Train the model first.\")\n",
    "\n",
    "    try:\n",
    "        # Preprocess input data as per training preprocessing\n",
    "        df = pd.DataFrame([data])\n",
    "\n",
    "        # Ensure numeric columns are properly formatted\n",
    "        numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "        df[numeric_columns] = df[numeric_columns].fillna(0).astype(float)\n",
    "\n",
    "        # Apply preprocessor transformations (same as during training)\n",
    "        processed_data = preprocessor.transform(df)\n",
    "\n",
    "        # Predict using the trained model\n",
    "        prediction = model.predict(processed_data)\n",
    "        return {\"prediction\": int(prediction[0])}\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "\n",
    "\n",
    "# Main function to run training and FastAPI\n",
    "def main():\n",
    "    df = pd.read_csv(\"football.csv\")  # Ensure your dataset exists\n",
    "\n",
    "    df, num_features, cat_features = preprocess_data(df)\n",
    "    X = df.drop(columns=['match_result'])\n",
    "    y = df['match_result']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Apply transformations\n",
    "    X_train = preprocessor.fit_transform(X_train)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "\n",
    "    # Train the model\n",
    "    model = train_model(X_train, y_train)\n",
    "    evaluate_model(model, X_test, y_test)\n",
    "    save_model_and_preprocessor(model, preprocessor)  # Save both model and preprocessor\n",
    "\n",
    "    # Run FastAPI\n",
    "    if \"ipykernel\" in sys.modules:  \n",
    "        from IPython.display import display, Javascript\n",
    "        display(Javascript('window.open(\"http://127.0.0.1:8000\")'))\n",
    "        uvicorn.run(\"ML:app\", host=\"0.0.0.0\", port=8000, reload=True)\n",
    "    else:\n",
    "        uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a3c8b4-c108-4325-8bf9-1cda8b6504de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#second Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccb051e-20ba-477e-9ea0-bd0bbb61162b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and preprocessor loaded successfully!\n",
      "Best Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best Accuracy: 0.9472131147540983\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.96      1.00      0.98        26\n",
      "           0       1.00      0.79      0.88        14\n",
      "           1       0.95      1.00      0.97        36\n",
      "\n",
      "    accuracy                           0.96        76\n",
      "   macro avg       0.97      0.93      0.94        76\n",
      "weighted avg       0.96      0.96      0.96        76\n",
      "\n",
      "Confusion Matrix:\n",
      " [[26  0  0]\n",
      " [ 1 11  2]\n",
      " [ 0  0 36]]\n",
      "Model Accuracy: 0.9605263157894737\n",
      "Model saved to model.pkl\n",
      "Preprocessor saved to preprocessor.pkl\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "window.open(\"http://127.0.0.1:8000\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Will watch for changes in these directories: ['C:\\\\Users\\\\Mnabah Tafetaleh\\\\nl']\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "INFO:     Started reloader process [10304] using StatReload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import uvicorn\n",
    "import pickle\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import os\n",
    "\n",
    "# Global preprocessor and model definition\n",
    "preprocessor = None\n",
    "model = None\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "def preprocess_data(df):\n",
    "    global preprocessor\n",
    "    # Drop unnecessary columns\n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(columns=[col for col in [\n",
    "        'timestamp', 'date_GMT', 'home_team_shots', 'over_35_percentage_pre_match', \n",
    "        'away_ppg', 'home_team_goal_count_half_time', 'over_25_percentage_pre_match', \n",
    "        'status', 'over_05_HT_FHG_percentage_pre_match', 'odds_ft_home_team_win', \n",
    "        'home_team_corner_count', 'odds_ft_over25', 'Away Team Pre-Match xG', \n",
    "        'Pre-Match PPG (Away)', 'away_team_first_half_cards', 'attendance.1', \n",
    "        'home_team_goal_timings', 'over_15_2HG_percentage_pre_match', \n",
    "        'odds_ft_away_team_win', 'home_ppg', 'Game Week', 'away_team_goal_timings', \n",
    "        'away_team_second_half_cards', 'Pre-Match PPG (Home)', 'attendance', \n",
    "        'odds_ft_draw', 'away_team_goal_count_half_time', 'btts_percentage_pre_match', \n",
    "        'odds_btts_yes', 'stadium_name', 'team_a_xg', 'home_team_first_half_cards', \n",
    "        'home_team_shots_off_target', 'Home Team Pre-Match xG', 'odds_ft_over35', \n",
    "        'odds_ft_over45', 'referee', 'home_team_second_half_cards', \n",
    "        'over_45_percentage_pre_match', 'away_team_name', 'away_team_corner_count', \n",
    "        'away_team_shots', 'over_05_2HG_percentage_pre_match', \n",
    "        'over_15_percentage_pre_match', 'average_cards_per_match_pre_match', \n",
    "        'average_corners_per_match_pre_match', 'odds_ft_over15', \n",
    "        'average_goals_per_match_pre_match', 'away_team_shots_off_target', \n",
    "        'total_goals_at_half_time', 'odds_btts_no', 'total_goal_count', 'team_b_xg', \n",
    "        'over_15_HT_FHG_percentage_pre_match','possession_difference', 'total_shots_on_target',\n",
    "    ] if col in df.columns])\n",
    "\n",
    "    # Creating new features\n",
    "    df['possession_difference'] = df['home_team_possession'] - df['away_team_possession']\n",
    "    df['total_shots_on_target'] = df['home_team_shots_on_target'] + df['away_team_shots_on_target']\n",
    "\n",
    "    # Handle missing values in numeric columns\n",
    "    numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "    df[numeric_columns] = df[numeric_columns].fillna(0)\n",
    "\n",
    "    # Convert all numeric columns to float (to handle mixed types)\n",
    "    df[numeric_columns] = df[numeric_columns].astype(float)\n",
    "\n",
    "    # Encoding match results as labels\n",
    "    df['match_result'] = df.apply(lambda row: 1 if row['home_team_goal_count'] > row['away_team_goal_count']\n",
    "                                  else (-1 if row['home_team_goal_count'] < row['away_team_goal_count'] else 0), axis=1)\n",
    "\n",
    "    # Select numerical and categorical features\n",
    "    numerical_features = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    numerical_features.remove('match_result')  # Exclude target variable\n",
    "\n",
    "    categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Initialize the preprocessor here\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "        ])\n",
    "    \n",
    "    return df, numerical_features, categorical_features\n",
    "\n",
    "\n",
    "# Step 2: Model Training\n",
    "def train_model(X_train, y_train):\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "    return best_model\n",
    "\n",
    "# Step 3: Model Evaluation\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Step 4: Save Model and Preprocessor\n",
    "def save_model_and_preprocessor(model, preprocessor, model_filename=\"model.pkl\", preprocessor_filename=\"preprocessor.pkl\"):\n",
    "    with open(model_filename, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    with open(preprocessor_filename, \"wb\") as f:\n",
    "        pickle.dump(preprocessor, f)\n",
    "    print(f\"Model saved to {model_filename}\")\n",
    "    print(f\"Preprocessor saved to {preprocessor_filename}\")\n",
    "\n",
    "# Step 5: FastAPI App\n",
    "app = FastAPI()\n",
    "\n",
    "# Load trained model and preprocessor\n",
    "if os.path.exists(\"model.pkl\") and os.path.exists(\"preprocessor.pkl\"):\n",
    "    with open(\"model.pkl\", \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    with open(\"preprocessor.pkl\", \"rb\") as f:\n",
    "        preprocessor = pickle.load(f)\n",
    "    print(\"Model and preprocessor loaded successfully!\")\n",
    "else:\n",
    "    print(\"Model or preprocessor not found! Please train the model first.\")\n",
    "    model = None\n",
    "    preprocessor = None\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "async def predict(data: dict):\n",
    "    if model is None:\n",
    "        raise HTTPException(status_code=500, detail=\"Model not loaded. Train the model first.\")\n",
    "    if preprocessor is None:\n",
    "        raise HTTPException(status_code=500, detail=\"Preprocessor not loaded. Train the model first.\")\n",
    "\n",
    "    try:\n",
    "        # Preprocess input data as per training preprocessing\n",
    "        df = pd.DataFrame([data])\n",
    "\n",
    "        # Ensure numeric columns are properly formatted\n",
    "        numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "        df[numeric_columns] = df[numeric_columns].fillna(0).astype(float)\n",
    "\n",
    "        # Apply preprocessor transformations (same as during training)\n",
    "        processed_data = preprocessor.transform(df)\n",
    "\n",
    "        # Predict using the trained model\n",
    "        prediction = model.predict(processed_data)\n",
    "        return {\"prediction\": int(prediction[0])}\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "\n",
    "\n",
    "@app.post(\"/predict_match/\")\n",
    "async def predict_match(data: dict):\n",
    "    if model is None or preprocessor is None:\n",
    "        raise HTTPException(status_code=500, detail=\"Model or preprocessor not loaded. Train the model first.\")\n",
    "\n",
    "    try:\n",
    "        # Ensure both teams' data is provided\n",
    "        if \"home_team\" not in data or \"away_team\" not in data:\n",
    "            raise HTTPException(status_code=400, detail=\"Both home_team and away_team data are required.\")\n",
    "\n",
    "        # Convert home and away team data into DataFrames\n",
    "        home_team_df = pd.DataFrame([data[\"home_team\"]])\n",
    "        away_team_df = pd.DataFrame([data[\"away_team\"]])\n",
    "\n",
    "        # Ensure all required columns exist\n",
    "        required_columns = {\n",
    "            \"home_team_possession\", \"home_team_shots_on_target\", \"home_team_yellow_cards\",\n",
    "            \"home_team_red_cards\", \"home_team_goal_count\", \"home_team_fouls\",\n",
    "            \"away_team_possession\", \"away_team_shots_on_target\", \"away_team_yellow_cards\",\n",
    "            \"away_team_red_cards\", \"away_team_goal_count\", \"away_team_fouls\"\n",
    "        }\n",
    "\n",
    "        # Check if required columns are missing\n",
    "        missing_columns = required_columns - set(home_team_df.columns) - set(away_team_df.columns)\n",
    "        if missing_columns:\n",
    "            raise HTTPException(status_code=400, detail=f\"Columns are missing: {missing_columns}\")\n",
    "\n",
    "        # Ensure numeric columns are properly formatted\n",
    "        numeric_columns_home = home_team_df.select_dtypes(include=['number']).columns\n",
    "        numeric_columns_away = away_team_df.select_dtypes(include=['number']).columns\n",
    "\n",
    "        home_team_df[numeric_columns_home] = home_team_df[numeric_columns_home].fillna(0).astype(float)\n",
    "        away_team_df[numeric_columns_away] = away_team_df[numeric_columns_away].fillna(0).astype(float)\n",
    "\n",
    "        # Merge both teams' data for prediction\n",
    "        match_df = home_team_df.copy()\n",
    "        match_df['away_team_possession'] = away_team_df['home_team_possession']\n",
    "        match_df['away_team_shots_on_target'] = away_team_df['home_team_shots_on_target']\n",
    "        match_df['away_team_yellow_cards'] = away_team_df['away_team_yellow_cards']\n",
    "        match_df['away_team_red_cards'] = away_team_df['away_team_red_cards']\n",
    "        match_df['away_team_goal_count'] = away_team_df['away_team_goal_count']\n",
    "        match_df['away_team_fouls'] = away_team_df['away_team_fouls']\n",
    "\n",
    "        match_df['possession_difference'] = match_df['home_team_possession'] - match_df['away_team_possession']\n",
    "        match_df['total_shots_on_target'] = match_df['home_team_shots_on_target'] + match_df['away_team_shots_on_target']\n",
    "        match_df['total_fouls'] = match_df['home_team_fouls'] + match_df['away_team_fouls']\n",
    "        match_df['total_yellow_cards'] = match_df['home_team_yellow_cards'] + match_df['away_team_yellow_cards']\n",
    "        match_df['total_red_cards'] = match_df['home_team_red_cards'] + match_df['away_team_red_cards']\n",
    "\n",
    "        # Apply preprocessor transformations\n",
    "        processed_data = preprocessor.transform(match_df)\n",
    "\n",
    "        # Predict match result\n",
    "        prediction = model.predict(processed_data)\n",
    "        result = \"Home Team Wins\" if prediction[0] == 1 else (\"Away Team Wins\" if prediction[0] == -1 else \"Draw\")\n",
    "\n",
    "        return {\"prediction\": result}\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "\n",
    "\n",
    "\n",
    "# Main function to run training and FastAPI\n",
    "def main():\n",
    "    df = pd.read_csv(\"football.csv\")  # Ensure your dataset exists\n",
    "\n",
    "    df, num_features, cat_features = preprocess_data(df)\n",
    "    X = df.drop(columns=['match_result'])\n",
    "    y = df['match_result']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Apply transformations\n",
    "    X_train = preprocessor.fit_transform(X_train)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "\n",
    "    # Train the model\n",
    "    model = train_model(X_train, y_train)\n",
    "    evaluate_model(model, X_test, y_test)\n",
    "    save_model_and_preprocessor(model, preprocessor)  # Save both model and preprocessor\n",
    "\n",
    "    # Run FastAPI\n",
    "    if \"ipykernel\" in sys.modules:  \n",
    "        from IPython.display import display, Javascript\n",
    "        display(Javascript('window.open(\"http://127.0.0.1:8000\")'))\n",
    "        uvicorn.run(\"ML:app\", host=\"0.0.0.0\", port=8000, reload=True)\n",
    "    else:\n",
    "        uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf7cbe9-3f7f-44d3-8833-6874665f931e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aece34b0-ee2e-4167-8940-d787f08415c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064a5630-6737-4e4f-a503-bd52461b7dab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bf6e3c-57fa-4e6b-938c-796954a309c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae14bdb1-3a30-4461-b90c-b39f2e1bf276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f51a214-47d9-4e0c-b838-7da9ba515cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5fb58e5-4711-4478-b2c4-7bd73f853f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not found! Please run training first.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 161\u001b[0m\n\u001b[0;32m    158\u001b[0m     uvicorn\u001b[38;5;241m.\u001b[39mrun(app, host\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.0.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m, port\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8000\u001b[39m)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 161\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[3], line 139\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    136\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfootball.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Replace with your dataset filename\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Preprocess data\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m df, preprocessor \u001b[38;5;241m=\u001b[39m preprocess_data(df)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Define features and target\u001b[39;00m\n\u001b[0;32m    142\u001b[0m X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatch_result\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[3], line 26\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     23\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_shots_on_target\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhome_team_shots_on_target\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maway_team_shots_on_target\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Handle Missing Values\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m df\u001b[38;5;241m.\u001b[39mfillna(df\u001b[38;5;241m.\u001b[39mmean(), inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     27\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatch_result\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhome_team_goal_count\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maway_team_goal_count\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhome_team_goal_count\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maway_team_goal_count\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Define numerical and categorical features\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:11693\u001b[0m, in \u001b[0;36mDataFrame.mean\u001b[1;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11685\u001b[0m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m  11686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[0;32m  11687\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11691\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  11692\u001b[0m ):\n\u001b[1;32m> 11693\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mmean(axis, skipna, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m  11694\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Series):\n\u001b[0;32m  11695\u001b[0m         result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:12420\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[1;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  12413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[0;32m  12414\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  12415\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  12418\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  12419\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m> 12420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stat_function(\n\u001b[0;32m  12421\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, nanops\u001b[38;5;241m.\u001b[39mnanmean, axis, skipna, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m  12422\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:12377\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[1;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  12373\u001b[0m nv\u001b[38;5;241m.\u001b[39mvalidate_func(name, (), kwargs)\n\u001b[0;32m  12375\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m> 12377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce(\n\u001b[0;32m  12378\u001b[0m     func, name\u001b[38;5;241m=\u001b[39mname, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only\n\u001b[0;32m  12379\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:11562\u001b[0m, in \u001b[0;36mDataFrame._reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m  11558\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m  11560\u001b[0m \u001b[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[0;32m  11561\u001b[0m \u001b[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[1;32m> 11562\u001b[0m res \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mreduce(blk_func)\n\u001b[0;32m  11563\u001b[0m out \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(res, axes\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m  11564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboolean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1500\u001b[0m, in \u001b[0;36mBlockManager.reduce\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m   1498\u001b[0m res_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m-> 1500\u001b[0m     nbs \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mreduce(func)\n\u001b[0;32m   1501\u001b[0m     res_blocks\u001b[38;5;241m.\u001b[39mextend(nbs)\n\u001b[0;32m   1503\u001b[0m index \u001b[38;5;241m=\u001b[39m Index([\u001b[38;5;28;01mNone\u001b[39;00m])  \u001b[38;5;66;03m# placeholder\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:404\u001b[0m, in \u001b[0;36mBlock.reduce\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduce\u001b[39m(\u001b[38;5;28mself\u001b[39m, func) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;66;03m# We will apply the function and reshape the result into a single-row\u001b[39;00m\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;66;03m#  Block with the same mgr_locs; squeezing will be done at a higher level\u001b[39;00m\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m--> 404\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    407\u001b[0m         res_values \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:11481\u001b[0m, in \u001b[0;36mDataFrame._reduce.<locals>.blk_func\u001b[1;34m(values, axis)\u001b[0m\n\u001b[0;32m  11479\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([result])\n\u001b[0;32m  11480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m> 11481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:147\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[1;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[0;32m    145\u001b[0m         result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:404\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[1;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[1;32m--> 404\u001b[0m result \u001b[38;5;241m=\u001b[39m func(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, mask\u001b[38;5;241m=\u001b[39mmask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n\u001b[0;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _wrap_results(result, orig_values\u001b[38;5;241m.\u001b[39mdtype, fill_value\u001b[38;5;241m=\u001b[39miNaT)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:719\u001b[0m, in \u001b[0;36mnanmean\u001b[1;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[0;32m    716\u001b[0m     dtype_count \u001b[38;5;241m=\u001b[39m dtype\n\u001b[0;32m    718\u001b[0m count \u001b[38;5;241m=\u001b[39m _get_counts(values\u001b[38;5;241m.\u001b[39mshape, mask, axis, dtype\u001b[38;5;241m=\u001b[39mdtype_count)\n\u001b[1;32m--> 719\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39msum(axis, dtype\u001b[38;5;241m=\u001b[39mdtype_sum)\n\u001b[0;32m    720\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m _ensure_numeric(the_sum)\n\u001b[0;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(the_sum, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:49\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "# ML.py\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import pickle\n",
    "from fastapi import FastAPI, HTTPException\n",
    "import logging\n",
    "import uvicorn\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the data: feature engineering, handling missing values, and encoding.\n",
    "    \"\"\"\n",
    "    # Feature Engineering\n",
    "    df['possession_difference'] = df['home_team_possession'] - df['away_team_possession']\n",
    "    df['total_shots_on_target'] = df['home_team_shots_on_target'] + df['away_team_shots_on_target']\n",
    "\n",
    "    # Handle Missing Values\n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "    df['match_result'] = df.apply(lambda row: 1 if row['home_team_goal_count'] > row['away_team_goal_count'] else (-1 if row['home_team_goal_count'] < row['away_team_goal_count'] else 0), axis=1)\n",
    "    # Define numerical and categorical features\n",
    "    numerical_features = ['home_team_possession', 'home_team_shots_on_target', \n",
    "                          'away_team_shots_on_target', 'home_team_corner_count', \n",
    "                          'away_team_corner_count', 'home_team_fouls', 'away_team_fouls',\n",
    "                          'possession_difference', 'total_shots_on_target']\n",
    "    categorical_features = []  # Add any categorical features if present\n",
    "\n",
    "    # Preprocessing pipeline\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(), categorical_features)\n",
    "        ])\n",
    "\n",
    "    return df, preprocessor\n",
    "\n",
    "# Step 2: Model Training and Hyperparameter Tuning\n",
    "def train_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train the model using GridSearchCV for hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    # Define the model\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "\n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "    return best_model\n",
    "\n",
    "# Step 3: Model Evaluation\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the model using classification report and confusion matrix.\n",
    "    \"\"\"\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Classification report\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    # Confusion matrix\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Model Accuracy:\", accuracy)\n",
    "\n",
    "# Step 4: Save the Model\n",
    "def save_model(model, filename=\"model.pkl\"):\n",
    "    \"\"\"\n",
    "    Save the trained model to a file.\n",
    "    \"\"\"\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Model saved to {filename}\")\n",
    "\n",
    "# Step 5: FastAPI Deployment\n",
    "app = FastAPI()\n",
    "\n",
    "import os\n",
    "\n",
    "# Load the trained model if it exists\n",
    "if os.path.exists(\"model.pkl\"):\n",
    "    with open(\"model.pkl\", \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    print(\"Model loaded successfully!\")\n",
    "else:\n",
    "    print(\"Model not found! Please run training first.\")\n",
    "    model = None  # Prevent errors in FastAPI if model isn't trained\n",
    "\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "async def predict(data: dict):\n",
    "    \"\"\"\n",
    "    Predict the match result using the trained model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert input into DataFrame\n",
    "        df = pd.DataFrame([data])\n",
    "        \n",
    "        # Predict result\n",
    "        prediction = model.predict(df)\n",
    "        \n",
    "        return {\"prediction\": int(prediction[0])}  # Convert NumPy output to integer\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during prediction: {e}\")\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "\n",
    "# Main function to run the entire pipeline\n",
    "def main():\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(\"football.csv\")  # Replace with your dataset filename\n",
    "\n",
    "    # Preprocess data\n",
    "    df, preprocessor = preprocess_data(df)\n",
    "\n",
    "    # Define features and target\n",
    "    X = df.drop(columns=['match_result'])\n",
    "    y = df['match_result']\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Train the model\n",
    "    model = train_model(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluate_model(model, X_test, y_test)\n",
    "\n",
    "    # Save the model\n",
    "    save_model(model)\n",
    "\n",
    "    # Run the FastAPI app\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f682e203-894a-472f-a3a3-def26396402a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 177)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<string>:177\u001b[1;36m\u001b[0m\n\u001b[1;33m    if \"ipykernel\" in sys.modules:\u001b[0m\n\u001b[1;37m                                  ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import pickle\n",
    "from fastapi import FastAPI, HTTPException\n",
    "import logging\n",
    "import uvicorn\n",
    "import sys\n",
    "import uvicorn\n",
    "import os\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "model = None  # Global model variable\n",
    "preprocessor = None  # Global preprocessor variable\n",
    "\n",
    "# ---------------- STEP 1: DATA PREPROCESSING ----------------\n",
    "def preprocess_data(df, fit_transform=True):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset: feature engineering, handling missing values, and encoding.\n",
    "    \"\"\"\n",
    "    # Feature Engineering\n",
    "    df['possession_difference'] = df['home_team_possession'] - df['away_team_possession']\n",
    "    df['total_shots_on_target'] = df['home_team_shots_on_target'] + df['away_team_shots_on_target']\n",
    "\n",
    "    # Convert numeric columns properly\n",
    "    numeric_cols = ['home_team_possession', 'home_team_shots_on_target', \n",
    "                    'away_team_shots_on_target', 'home_team_corner_count', \n",
    "                    'away_team_corner_count', 'home_team_fouls', 'away_team_fouls',\n",
    "                    'possession_difference', 'total_shots_on_target']\n",
    "    \n",
    "    df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    df.fillna(df[numeric_cols].mean(), inplace=True)  # Fill only numeric columns\n",
    "\n",
    "    # Define match result target\n",
    "    df['match_result'] = df.apply(lambda row: 1 if row['home_team_goal_count'] > row['away_team_goal_count'] \n",
    "                                  else (-1 if row['home_team_goal_count'] < row['away_team_goal_count'] else 0), axis=1)\n",
    "\n",
    "    # Define preprocessing pipeline\n",
    "    numerical_features = numeric_cols\n",
    "    categorical_features = []  # Add any categorical features if needed\n",
    "\n",
    "    global preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(), categorical_features)\n",
    "        ])\n",
    "\n",
    "    if fit_transform:\n",
    "        X = df.drop(columns=['match_result'])\n",
    "        X = preprocessor.fit_transform(X)  # Fit and transform during training\n",
    "    else:\n",
    "        X = preprocessor.transform(df)  # Only transform during prediction\n",
    "\n",
    "    return X, df['match_result']\n",
    "\n",
    "# ---------------- STEP 2: MODEL TRAINING ----------------\n",
    "def train_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train the model using GridSearchCV for hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    # Define the model\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "\n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "    return best_model\n",
    "\n",
    "# ---------------- STEP 3: MODEL EVALUATION ----------------\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the model using classification report and confusion matrix.\n",
    "    \"\"\"\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# ---------------- STEP 4: SAVE & LOAD MODEL ----------------\n",
    "def save_model(model, preprocessor, filename=\"model.pkl\"):\n",
    "    \"\"\"\n",
    "    Save the trained model and preprocessor to a file.\n",
    "    \"\"\"\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump((model, preprocessor), f)\n",
    "    print(f\"Model and Preprocessor saved to {filename}\")\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Load the trained model and preprocessor from file.\n",
    "    \"\"\"\n",
    "    global model, preprocessor\n",
    "    if os.path.exists(\"model.pkl\"):\n",
    "        with open(\"model.pkl\", \"rb\") as f:\n",
    "            model, preprocessor = pickle.load(f)\n",
    "        print(\"Model loaded successfully!\")\n",
    "    else:\n",
    "        print(\"Model not found! Please train the model first.\")\n",
    "\n",
    "# ---------------- FASTAPI ENDPOINTS ----------------\n",
    "@app.on_event(\"startup\")\n",
    "def startup_event():\n",
    "    \"\"\"\n",
    "    Load the model on FastAPI startup.\n",
    "    \"\"\"\n",
    "    load_model()\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "async def predict(data: dict):\n",
    "    \"\"\"\n",
    "    Predict the match result using the trained model.\n",
    "    \"\"\"\n",
    "    if model is None or preprocessor is None:\n",
    "        raise HTTPException(status_code=500, detail=\"Model not loaded. Train the model first.\")\n",
    "    \n",
    "    try:\n",
    "        # Convert input into DataFrame\n",
    "        df = pd.DataFrame([data])\n",
    "\n",
    "        # Transform input using the same preprocessing pipeline\n",
    "        X_input = preprocessor.transform(df)\n",
    "\n",
    "        # Predict result\n",
    "        prediction = model.predict(X_input)\n",
    "\n",
    "        return {\"prediction\": int(prediction[0])}  # Convert NumPy output to integer\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during prediction: {e}\")\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "\n",
    "# ---------------- MAIN FUNCTION ----------------\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Load data, preprocess, train, evaluate, and save the model.\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(\"football.csv\")  # Ensure this file exists\n",
    "\n",
    "    # Preprocess data\n",
    "    X, y = preprocess_data(df)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Train the model\n",
    "    model = train_model(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluate_model(model, X_test, y_test)\n",
    "\n",
    "    # Save the model and preprocessor\n",
    "    save_model(model, preprocessor)\n",
    "\n",
    "    # Run FastAPI\n",
    "   if \"ipykernel\" in sys.modules:\n",
    "            # Running in Jupyter, start Uvicorn another way\n",
    "            from IPython.display import display, Javascript\n",
    "            display(Javascript('window.open(\"http://127.0.0.1:8000\")'))\n",
    "            !uvicorn ML:app --reload\n",
    "        else:\n",
    "            uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9c0871-9e4b-41d5-a720-bb64b6aacd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import uvicorn\n",
    "import pickle\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import os\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "def preprocess_data(df):\n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(columns=[col for col in ['timestamp', 'date_GMT'] if col in df.columns])\n",
    "\n",
    "    # Creating new features\n",
    "    df['possession_difference'] = df['home_team_possession'] - df['away_team_possession']\n",
    "    df['total_shots_on_target'] = df['home_team_shots_on_target'] + df['away_team_shots_on_target']\n",
    "\n",
    "    # Handle missing values\n",
    "    df.fillna(df.mean(numeric_only=True), inplace=True)\n",
    "\n",
    "    # Encoding match results as labels\n",
    "    df['match_result'] = df.apply(lambda row: 1 if row['home_team_goal_count'] > row['away_team_goal_count']\n",
    "                                  else (-1 if row['home_team_goal_count'] < row['away_team_goal_count'] else 0), axis=1)\n",
    "\n",
    "    # Select numerical and categorical features\n",
    "    numerical_features = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    numerical_features.remove('match_result')  # Exclude target variable\n",
    "\n",
    "    categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "        ])\n",
    "\n",
    "    return df, preprocessor, numerical_features, categorical_features\n",
    "\n",
    "\n",
    "# Step 2: Model Training\n",
    "def train_model(X_train, y_train):\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "    return best_model\n",
    "\n",
    "# Step 3: Model Evaluation\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Step 4: Save Model\n",
    "def save_model(model, filename=\"model.pkl\"):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Model saved to {filename}\")\n",
    "\n",
    "# Step 5: FastAPI App\n",
    "app = FastAPI()\n",
    "\n",
    "# Load trained model\n",
    "if os.path.exists(\"model.pkl\"):\n",
    "    with open(\"model.pkl\", \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    print(\"Model loaded successfully!\")\n",
    "else:\n",
    "    print(\"Model not found! Please train it first.\")\n",
    "    model = None  \n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "async def predict(data: dict):\n",
    "    if model is None:\n",
    "        raise HTTPException(status_code=500, detail=\"Model not loaded. Train the model first.\")\n",
    "\n",
    "    try:\n",
    "        df = pd.DataFrame([data])\n",
    "        prediction = model.predict(df)\n",
    "        return {\"prediction\": int(prediction[0])}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "\n",
    "# Main function to run training and FastAPI\n",
    "def main():\n",
    "    df = pd.read_csv(\"football.csv\")  # Ensure your dataset exists\n",
    "\n",
    "     \n",
    "    df, preprocessor, num_features, cat_features = preprocess_data(df)\n",
    "    X = df.drop(columns=['match_result'])\n",
    "    y = df['match_result']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Apply transformations\n",
    "    X_train = preprocessor.fit_transform(X_train)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "\n",
    "    # Train the model\n",
    "    model = train_model(X_train, y_train)\n",
    "    evaluate_model(model, X_test, y_test)\n",
    "    save_model(model)\n",
    "\n",
    "    # Run FastAPI\n",
    "    if \"ipykernel\" in sys.modules:  \n",
    "        from IPython.display import display, Javascript\n",
    "        display(Javascript('window.open(\"http://127.0.0.1:8000\")'))\n",
    "        !uvicorn ML:app --reload  \n",
    "    else:\n",
    "        uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa90784-fd7b-4613-aeed-e3669c931016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from fastapi import FastAPI\n",
    "import asyncio\n",
    "\n",
    "# Apply nest_asyncio to allow running asyncio event loop inside Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Define FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"Hello, World!\"}\n",
    "\n",
    "# Define a prediction endpoint (you can use your own)\n",
    "@app.post(\"/predict/\")\n",
    "async def predict(data: dict):\n",
    "    return {\"prediction\": \"Your prediction result here\"}\n",
    "\n",
    "# Run the server using uvicorn, within the notebook\n",
    "uvicorn.run(app, host=\"127.0.0.1\", port=8000, loop=\"asyncio\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603b311d-294b-41f0-af8e-c79f93cc805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uvicorn ML:app --reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794c841a-1fb5-4ff7-b187-bb6d5d57fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import uvicorn\n",
    "import pickle\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import os\n",
    "\n",
    "# Global preprocessor and model definition\n",
    "preprocessor = None\n",
    "model = None\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "def preprocess_data(df):\n",
    "    global preprocessor\n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(columns=[col for col in ['timestamp', 'date_GMT'] if col in df.columns])\n",
    "\n",
    "    # Creating new features\n",
    "    df['possession_difference'] = df['home_team_possession'] - df['away_team_possession']\n",
    "    df['total_shots_on_target'] = df['home_team_shots_on_target'] + df['away_team_shots_on_target']\n",
    "\n",
    "    # Handle missing values\n",
    "    df.fillna(df.mean(numeric_only=True), inplace=True)\n",
    "\n",
    "    # Encoding match results as labels\n",
    "    df['match_result'] = df.apply(lambda row: 1 if row['home_team_goal_count'] > row['away_team_goal_count']\n",
    "                                  else (-1 if row['home_team_goal_count'] < row['away_team_goal_count'] else 0), axis=1)\n",
    "\n",
    "    # Select numerical and categorical features\n",
    "    numerical_features = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    numerical_features.remove('match_result')  # Exclude target variable\n",
    "\n",
    "    categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Initialize the preprocessor here\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "        ])\n",
    "    \n",
    "    return df, numerical_features, categorical_features\n",
    "\n",
    "\n",
    "# Step 2: Model Training\n",
    "def train_model(X_train, y_train):\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "    return best_model\n",
    "\n",
    "# Step 3: Model Evaluation\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Step 4: Save Model\n",
    "def save_model(model, filename=\"model.pkl\"):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Model saved to {filename}\")\n",
    "\n",
    "# Step 5: FastAPI App\n",
    "app = FastAPI()\n",
    "\n",
    " \n",
    "# Load trained model\n",
    "if os.path.exists(\"model.pkl\"):\n",
    "    with open(\"model.pkl\", \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    print(\"Model loaded successfully!\")\n",
    "\n",
    "    # Reinitialize the preprocessor\n",
    "    if model is not None:\n",
    "        df = pd.read_csv(\"football.csv\")  # Reload the training data for re-initializing preprocessor\n",
    "        df, num_features, cat_features = preprocess_data(df)\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), num_features),\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
    "            ])\n",
    "        print(\"Preprocessor initialized successfully!\")\n",
    "else:\n",
    "    print(\"Model not found! Please train it first.\")\n",
    "    model = None\n",
    "    preprocessor = None\n",
    " \n",
    "\n",
    "# Load preprocessor globally\n",
    "if preprocessor is None:\n",
    "    # You should reinitialize your preprocessor from the training phase if it's missing\n",
    "    print(\"Preprocessor not initialized! Ensure to train and save the model.\")\n",
    "    preprocessor = None\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "async def predict(data: dict):\n",
    "    if model is None:\n",
    "        raise HTTPException(status_code=500, detail=\"Model not loaded. Train the model first.\")\n",
    "    if preprocessor is None:\n",
    "        raise HTTPException(status_code=500, detail=\"Preprocessor not loaded. Train the model first.\")\n",
    "\n",
    "    try:\n",
    "        # Preprocess input data as per training preprocessing\n",
    "        df = pd.DataFrame([data])\n",
    "\n",
    "        # Apply preprocessor transformations (same as during training)\n",
    "        processed_data = preprocessor.transform(df)\n",
    "\n",
    "        # Predict using the trained model\n",
    "        prediction = model.predict(processed_data)\n",
    "        return {\"prediction\": int(prediction[0])}\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "\n",
    "\n",
    "# Main function to run training and FastAPI\n",
    "def main():\n",
    "    df = pd.read_csv(\"football.csv\")  # Ensure your dataset exists\n",
    "\n",
    "    df, num_features, cat_features = preprocess_data(df)\n",
    "    X = df.drop(columns=['match_result'])\n",
    "    y = df['match_result']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Apply transformations\n",
    "    X_train = preprocessor.fit_transform(X_train)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "\n",
    "    # Train the model\n",
    "    model = train_model(X_train, y_train)\n",
    "    evaluate_model(model, X_test, y_test)\n",
    "    save_model(model)\n",
    "\n",
    "    # Run FastAPI\n",
    "    if \"ipykernel\" in sys.modules:  \n",
    "        from IPython.display import display, Javascript\n",
    "        display(Javascript('window.open(\"http://127.0.0.1:8000\")'))\n",
    "        uvicorn.run(\"ML:app\", host=\"0.0.0.0\", port=8000, reload=True)\n",
    "    else:\n",
    "        uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce3563-803a-422d-8f79-231505780df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7230617f-2ace-4b0c-806b-76a3e46e1e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model or Preprocessor not found! Please train them first.\n",
      "Preprocessor not initialized! Ensure to train and save the model.\n",
      "Best Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best Accuracy: 0.8355191256830601\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.89      0.96      0.93        26\n",
      "           0       1.00      0.43      0.60        14\n",
      "           1       0.86      1.00      0.92        36\n",
      "\n",
      "    accuracy                           0.88        76\n",
      "   macro avg       0.92      0.80      0.82        76\n",
      "weighted avg       0.90      0.88      0.86        76\n",
      "\n",
      "Confusion Matrix:\n",
      " [[25  0  1]\n",
      " [ 3  6  5]\n",
      " [ 0  0 36]]\n",
      "Model Accuracy: 0.881578947368421\n",
      "Model saved to model.pkl\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "window.open(\"http://127.0.0.1:8000\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Will watch for changes in these directories: ['C:\\\\Users\\\\Mnabah Tafetaleh\\\\nl']\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "INFO:     Started reloader process [372] using StatReload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import uvicorn\n",
    "import pickle\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import os\n",
    "\n",
    "# Global preprocessor and model definition\n",
    "preprocessor = None\n",
    "model = None\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "def preprocess_data(df):\n",
    "    global preprocessor\n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(columns=[col for col in ['timestamp', 'date_GMT'] if col in df.columns])\n",
    "\n",
    "    # Creating new features\n",
    "    df['possession_difference'] = df['home_team_possession'] - df['away_team_possession']\n",
    "    df['total_shots_on_target'] = df['home_team_shots_on_target'] + df['away_team_shots_on_target']\n",
    "\n",
    "    # Handle missing values\n",
    "    df.fillna(df.mean(numeric_only=True), inplace=True)\n",
    "\n",
    "    # Encoding match results as labels\n",
    "    df['match_result'] = df.apply(lambda row: 1 if row['home_team_goal_count'] > row['away_team_goal_count']\n",
    "                                  else (-1 if row['home_team_goal_count'] < row['away_team_goal_count'] else 0), axis=1)\n",
    "\n",
    "    # Select numerical and categorical features\n",
    "    numerical_features = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    numerical_features.remove('match_result')  # Exclude target variable\n",
    "\n",
    "    categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Initialize the preprocessor here\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "        ])\n",
    "    \n",
    "    return df, numerical_features, categorical_features\n",
    "\n",
    "\n",
    "# Step 2: Model Training\n",
    "def train_model(X_train, y_train):\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "    return best_model\n",
    "\n",
    "# Step 3: Model Evaluation\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Step 4: Save Model\n",
    "def save_model(model, filename=\"model.pkl\"):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Model saved to {filename}\")\n",
    "\n",
    "# Step 5: FastAPI App\n",
    "app = FastAPI()\n",
    "\n",
    " \n",
    "# Load trained model\n",
    "# Load trained model and preprocessor\n",
    "if os.path.exists(\"model.pkl\") and os.path.exists(\"preprocessor.pkl\"):\n",
    "    with open(\"model.pkl\", \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    print(\"Model loaded successfully!\")\n",
    "\n",
    "    # Load preprocessor and fit it if necessary\n",
    "    with open(\"preprocessor.pkl\", \"rb\") as f:\n",
    "        preprocessor = pickle.load(f)\n",
    "    print(\"Preprocessor loaded successfully!\")\n",
    "\n",
    "    # Check if the preprocessor is already fitted (this is optional, just for safety)\n",
    "    if not hasattr(preprocessor, 'transform'):\n",
    "        raise HTTPException(status_code=500, detail=\"Preprocessor not fitted. Train the model first.\")\n",
    "else:\n",
    "    print(\"Model or Preprocessor not found! Please train them first.\")\n",
    "    model = None\n",
    "    preprocessor = None\n",
    "\n",
    " \n",
    "\n",
    "# Load preprocessor globally\n",
    "if preprocessor is None:\n",
    "    # You should reinitialize your preprocessor from the training phase if it's missing\n",
    "    print(\"Preprocessor not initialized! Ensure to train and save the model.\")\n",
    "    preprocessor = None\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "async def predict(data: dict):\n",
    "    if model is None:\n",
    "        raise HTTPException(status_code=500, detail=\"Model not loaded. Train the model first.\")\n",
    "    if preprocessor is None:\n",
    "        raise HTTPException(status_code=500, detail=\"Preprocessor not loaded. Train the model first.\")\n",
    "\n",
    "    try:\n",
    "        # Preprocess input data as per training preprocessing\n",
    "        df = pd.DataFrame([data])\n",
    "\n",
    "        # Apply preprocessor transformations (same as during training)\n",
    "        processed_data = preprocessor.transform(df)\n",
    "\n",
    "        # Predict using the trained model\n",
    "        prediction = model.predict(processed_data)\n",
    "        return {\"prediction\": int(prediction[0])}\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "\n",
    "\n",
    "# Main function to run training and FastAPI\n",
    "def main():\n",
    "    df = pd.read_csv(\"football.csv\")  # Ensure your dataset exists\n",
    "\n",
    "    df, num_features, cat_features = preprocess_data(df)\n",
    "    X = df.drop(columns=['match_result'])\n",
    "    y = df['match_result']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Apply transformations\n",
    "    X_train = preprocessor.fit_transform(X_train)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "\n",
    "    # Train the model\n",
    "    model = train_model(X_train, y_train)\n",
    "    evaluate_model(model, X_test, y_test)\n",
    "    save_model(model)\n",
    "\n",
    "    # Run FastAPI\n",
    "    if \"ipykernel\" in sys.modules:  \n",
    "        from IPython.display import display, Javascript\n",
    "        display(Javascript('window.open(\"http://127.0.0.1:8000\")'))\n",
    "        uvicorn.run(\"ML:app\", host=\"0.0.0.0\", port=8000, reload=True)\n",
    "    else:\n",
    "        uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe6c51-b182-4185-837f-0b59b899a254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
